{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import smtplib\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import math\n",
    "import difflib\n",
    "import requests\n",
    "import os\n",
    "import cv2  # Import OpenCV\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Specify the path to the geckodriver executable\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "import json\n",
    "\n",
    "amazon_titles= []\n",
    "amazon_links = []\n",
    "webuy_titles = []\n",
    "webuy_links = []\n",
    "amazon_images = []\n",
    "webuy_images = []\n",
    "calculated_prices = []\n",
    "product_codes = []\n",
    "buy_prices = []\n",
    "\n",
    "with open(\"WorkingProxies.txt\", \"r\", encoding=\"utf-8\") as prxsfile:\n",
    "    proxy_list = prxsfile.read().split(\"\\n\")\n",
    "def get_random_proxy():\n",
    "    \"\"\"Select a random proxy from the list.\"\"\"\n",
    "    return random.choice(proxy_list)\n",
    "\n",
    "def create_driver_with_proxy(proxy):\n",
    "    \"\"\"Create a new Chrome WebDriver instance with a proxy.\"\"\"\n",
    "    # Split the proxy IP and port\n",
    "    proxy_ip, proxy_port = proxy.split(':')\n",
    "    \n",
    "    # Create Chrome options\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Configure proxy settings for both HTTP and HTTPS\n",
    "    proxy_settings = f\"{proxy_ip}:{proxy_port}\"\n",
    "    chrome_options.add_argument(f'--proxy-server={proxy_settings}')\n",
    "    \n",
    "    # Specify the path to the Chrome executable if needed (optional)\n",
    "    # chrome_options.binary_location = \"C:\\\\Path\\\\To\\\\Chrome.exe\"  # Only needed if Chrome is not in PATH\n",
    "    \n",
    "    # Initialize the WebDriver with the options\n",
    "    amazon_driver = webdriver.Chrome(options=chrome_options)\n",
    "    return amazon_driver\n",
    "\n",
    "def create_driver_without_proxy():\n",
    "    \"\"\"Create a new Chrome WebDriver instance without a proxy.\"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the WebDriver with the options\n",
    "    driver = webdriver.Chrome()\n",
    "    return driver\n",
    "def wait_for_element_to_be_clickable(driver, xpath, timeout=10):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath))\n",
    "    )\n",
    "\n",
    "def check_blacklisted_sellers(sellers_list, sellers_black_list):\n",
    "    # Find the blacklisted sellers in the sellers list\n",
    "    blacklisted_sellers = [seller for seller in sellers_list if seller in sellers_black_list]\n",
    "\n",
    "    if blacklisted_sellers:\n",
    "        print(\"Not a valid product\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No sellers in the list are blacklisted.\")\n",
    "        return False\n",
    "\n",
    "def check_whitelisted_sellers(sellers_list, seller_white_list):\n",
    "    # Loop through the sellers list to find the first whitelisted seller\n",
    "    for index, seller in enumerate(sellers_list):\n",
    "        if seller in seller_white_list:\n",
    "            print(\"First whitelisted seller found at index:\", index)\n",
    "            return index  # Return the index of the first whitelisted seller\n",
    "    \n",
    "    print(\"No sellers in the list are whitelisted.\")\n",
    "    return -1  # Return -1 if no whitelisted seller is found\n",
    "\n",
    "def calculate_percentages(title_data, image_data):\n",
    "    results = []\n",
    "    \n",
    "    # Ensure both lists have the same length\n",
    "    if len(title_data) != len(image_data):\n",
    "        raise ValueError(\"The length of title_data and image_data must be the same.\")\n",
    "    \n",
    "    for (title, title_percentage), (image, image_percentage) in zip(title_data, image_data):\n",
    "        new_percentage = (0.8 * image_percentage) + (0.2 * title_percentage)\n",
    "        results.append(new_percentage)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_similarity(reference_string, string_list):\n",
    "    similarity_scores = []\n",
    "    \n",
    "    for string in string_list:\n",
    "        # Calculate similarity ratio\n",
    "        similarity_ratio = difflib.SequenceMatcher(None, reference_string, string).ratio()\n",
    "        # Convert to percentage\n",
    "        similarity_percentage = similarity_ratio * 100\n",
    "        similarity_scores.append((string, similarity_percentage))\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "def download_reference_image(url, download_dir='images\\\\reference'):\n",
    "    \"\"\"Download the reference image to a specified directory.\"\"\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        image_name = os.path.join(download_dir, url.split(\"/\")[-1])\n",
    "        with open(image_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return image_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading reference image {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"Calculate SSIM manually.\"\"\"\n",
    "    # Constants to avoid division by zero\n",
    "    C1 = 6.5025\n",
    "    C2 = 58.5225\n",
    "\n",
    "    # Calculate means\n",
    "    mu1 = np.mean(img1)\n",
    "    mu2 = np.mean(img2)\n",
    "\n",
    "    # Calculate variances\n",
    "    sigma1_sq = np.var(img1)\n",
    "    sigma2_sq = np.var(img2)\n",
    "    sigma12 = np.cov(img1.flatten(), img2.flatten())[0][1]\n",
    "\n",
    "    # SSIM formula\n",
    "    ssim = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "            ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    return ssim\n",
    "\n",
    "\n",
    "def compare_images(image1_path, image2_path):\n",
    "    \"\"\"Compare two images and return similarity percentage using OpenCV.\"\"\"\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    img2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "\n",
    "    # Resize images to the same size for comparison\n",
    "    img1 = cv2.resize(img1, (100, 100))  # Adjust size as necessary\n",
    "    img2 = cv2.resize(img2, (100, 100))  # Adjust size as necessary\n",
    "\n",
    "    # Calculate SSIM\n",
    "    ssim_score = calculate_ssim(img1, img2)\n",
    "    \n",
    "    # Convert to percentage\n",
    "    similarity_percentage = ssim_score * 100\n",
    "    return similarity_percentage\n",
    "\n",
    "\n",
    "def clear_temp_images(download_dir='images\\\\temporary_images'):\n",
    "    \"\"\"Clear all images in the temporary images directory.\"\"\"\n",
    "    if os.path.exists(download_dir):\n",
    "        for filename in os.listdir(download_dir):\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    \n",
    "                    os.remove(file_path)  # Remove the file\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"The directory {download_dir} does not exist.\")\n",
    "\n",
    "def download_images(image_urls, download_dir='images\\\\temporary_images'):\n",
    "    \"\"\"Download images from the given URLs.\"\"\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    downloaded_images = []\n",
    "    for url in image_urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an error for bad responses\n",
    "            image_name = os.path.join(download_dir, url.split(\"/\")[-1])\n",
    "            with open(image_name, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            downloaded_images.append(image_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "    return downloaded_images\n",
    "\n",
    "def check_black_list_words(word_list, target_string):\n",
    "    # Convert the target string to lowercase\n",
    "    target_string = target_string.lower()\n",
    "\n",
    "    # Loop through each word in the list and check in lowercase\n",
    "    for word in word_list:\n",
    "        if word.lower() in target_string:\n",
    "            return True\n",
    "    return False\n",
    "def check_prices_more_than_webuyPrice(amazon_prices, webuyprice):\n",
    "    # Initialize a counter for numbers less than the threshold\n",
    "    count = 0\n",
    "\n",
    "    # Loop through each number in the list\n",
    "    for number in amazon_prices:\n",
    "        if number < webuyprice:\n",
    "            count += 1\n",
    "        # If the count reaches 3, return True immediately\n",
    "        if count >= 3:\n",
    "            return True\n",
    "\n",
    "    # If the loop finishes and less than 3 numbers are found, return False\n",
    "    return False\n",
    "\n",
    "def download_webuy_image(url, download_dir='images\\\\webuy_images'):\n",
    "    \"\"\"Download the reference image to a specified directory.\"\"\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        image_name = os.path.join(download_dir, url.split(\"/\")[-1])\n",
    "        with open(image_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return image_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading reference image {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "sellers_black_list = [\" Live Life Retail\"]\n",
    "seller_white_list = [\" Selling-Prime-UK\"]\n",
    "words_black_list =  [\"ntsc\", \"renewed\", \"reproduction\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7376BB095+29557]\n",
      "\t(No symbol) [0x00007FF73762FA50]\n",
      "\t(No symbol) [0x00007FF7374EB56A]\n",
      "\t(No symbol) [0x00007FF73753F695]\n",
      "\t(No symbol) [0x00007FF73753F8EC]\n",
      "\t(No symbol) [0x00007FF73758B777]\n",
      "\t(No symbol) [0x00007FF7375671CF]\n",
      "\t(No symbol) [0x00007FF73758851C]\n",
      "\t(No symbol) [0x00007FF737566F33]\n",
      "\t(No symbol) [0x00007FF73753116F]\n",
      "\t(No symbol) [0x00007FF7375322D1]\n",
      "\tGetHandleVerifier [0x00007FF7379EC96D+3378253]\n",
      "\tGetHandleVerifier [0x00007FF737A38497+3688311]\n",
      "\tGetHandleVerifier [0x00007FF737A2D1CB+3642539]\n",
      "\tGetHandleVerifier [0x00007FF73777A6B6+813462]\n",
      "\t(No symbol) [0x00007FF73763AB5F]\n",
      "\t(No symbol) [0x00007FF737636B74]\n",
      "\t(No symbol) [0x00007FF737636D10]\n",
      "\t(No symbol) [0x00007FF737625C1F]\n",
      "\tBaseThreadInitThunk [0x00007FFFEB127374+20]\n",
      "\tRtlUserThreadStart [0x00007FFFECE5CC91+33]\n",
      "\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF7376BB095+29557]\n\t(No symbol) [0x00007FF73762FA50]\n\t(No symbol) [0x00007FF7374EB56A]\n\t(No symbol) [0x00007FF73753F695]\n\t(No symbol) [0x00007FF73753F8EC]\n\t(No symbol) [0x00007FF73758B777]\n\t(No symbol) [0x00007FF7375671CF]\n\t(No symbol) [0x00007FF73758851C]\n\t(No symbol) [0x00007FF737566F33]\n\t(No symbol) [0x00007FF73753116F]\n\t(No symbol) [0x00007FF7375322D1]\n\tGetHandleVerifier [0x00007FF7379EC96D+3378253]\n\tGetHandleVerifier [0x00007FF737A38497+3688311]\n\tGetHandleVerifier [0x00007FF737A2D1CB+3642539]\n\tGetHandleVerifier [0x00007FF73777A6B6+813462]\n\t(No symbol) [0x00007FF73763AB5F]\n\t(No symbol) [0x00007FF737636B74]\n\t(No symbol) [0x00007FF737636D10]\n\t(No symbol) [0x00007FF737625C1F]\n\tBaseThreadInitThunk [0x00007FFFEB127374+20]\n\tRtlUserThreadStart [0x00007FFFECE5CC91+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Click the \"Deliver to\" button\u001b[39;00m\n\u001b[0;32m     25\u001b[0m deliver_to_button_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglow-ingress-line2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mwait_for_element_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamazon_driver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeliver_to_button_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Click the Post Code input field\u001b[39;00m\n\u001b[0;32m     29\u001b[0m PostCode_input_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLUXZipUpdateInput\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m, in \u001b[0;36mwait_for_element_to_be_clickable\u001b[1;34m(driver, xpath, timeout)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait_for_element_to_be_clickable\u001b[39m(driver, xpath, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_to_be_clickable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\black2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x00007FF7376BB095+29557]\n\t(No symbol) [0x00007FF73762FA50]\n\t(No symbol) [0x00007FF7374EB56A]\n\t(No symbol) [0x00007FF73753F695]\n\t(No symbol) [0x00007FF73753F8EC]\n\t(No symbol) [0x00007FF73758B777]\n\t(No symbol) [0x00007FF7375671CF]\n\t(No symbol) [0x00007FF73758851C]\n\t(No symbol) [0x00007FF737566F33]\n\t(No symbol) [0x00007FF73753116F]\n\t(No symbol) [0x00007FF7375322D1]\n\tGetHandleVerifier [0x00007FF7379EC96D+3378253]\n\tGetHandleVerifier [0x00007FF737A38497+3688311]\n\tGetHandleVerifier [0x00007FF737A2D1CB+3642539]\n\tGetHandleVerifier [0x00007FF73777A6B6+813462]\n\t(No symbol) [0x00007FF73763AB5F]\n\t(No symbol) [0x00007FF737636B74]\n\t(No symbol) [0x00007FF737636D10]\n\t(No symbol) [0x00007FF737625C1F]\n\tBaseThreadInitThunk [0x00007FFFEB127374+20]\n\tRtlUserThreadStart [0x00007FFFECE5CC91+33]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rotate proxies\n",
    "high_price = 30\n",
    "low_price = 25\n",
    "proxy = get_random_proxy()  # Get a random proxy\n",
    "#driver = create_driver_with_proxy(proxy)  # Create WebDriver with that proxy\n",
    "amazon_driver = create_driver_without_proxy()\n",
    "webuy_driver = create_driver_without_proxy()\n",
    "url = f\"https://www.amazon.co.uk/\"\n",
    "try:\n",
    "    amazon_driver.get(url)\n",
    "except Exception:\n",
    "    pass\n",
    "amazon_cookies_PATH = '/html/body/div[2]/span/form/div[2]/span[1]/span/input'\n",
    "try:\n",
    "    # Wait until the element is clickable\n",
    "    WebDriverWait(amazon_driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, amazon_cookies_PATH))\n",
    "    )\n",
    "    amazon_driver.find_element(By.XPATH, amazon_cookies_PATH).click()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Click the \"Deliver to\" button\n",
    "deliver_to_button_PATH = '//*[@id=\"glow-ingress-line2\"]'\n",
    "wait_for_element_to_be_clickable(amazon_driver, deliver_to_button_PATH).click()\n",
    "\n",
    "# Click the Post Code input field\n",
    "PostCode_input_PATH = '//*[@id=\"GLUXZipUpdateInput\"]'\n",
    "wait_for_element_to_be_clickable(amazon_driver, PostCode_input_PATH).click()\n",
    "\n",
    "# Enter the postcode\n",
    "amazon_driver.find_element(By.XPATH, PostCode_input_PATH).send_keys(\"SW1A2aa\")\n",
    "\n",
    "# Click the \"Apply\" button\n",
    "Apply_button_PATH = '//*[@id=\"GLUXZipUpdate\"]/span/input'\n",
    "wait_for_element_to_be_clickable(amazon_driver, Apply_button_PATH).click()\n",
    "\n",
    "# Click the \"Continue\" button\n",
    "continue_button_PATH = '/html/body/div[5]/div/div/div[2]/span/span/input'\n",
    "wait_for_element_to_be_clickable(amazon_driver, continue_button_PATH).click()\n",
    "\n",
    "# Click to accept cookies\n",
    "try:\n",
    "\n",
    "    accept_cookies_PATH = '/html/body/div[1]/span/form/div[2]/span[1]/span/input'\n",
    "    wait_for_element_to_be_clickable(amazon_driver, accept_cookies_PATH).click()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_titles= []\n",
    "amazon_links = []\n",
    "webuy_titles = []\n",
    "webuy_links = []\n",
    "amazon_images = []\n",
    "webuy_images = []\n",
    "calculated_prices = []\n",
    "product_codes = []\n",
    "buy_prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final high price: 26.25, Final low price: 25.0, Number of products: 92\n",
      "Number of pages: 6\n",
      "Page 1 processed. Number of product links found: 16\n",
      "Page 2 processed. Number of product links found: 16\n",
      "Page 3 processed. Number of product links found: 16\n",
      "Page 4 processed. Number of product links found: 16\n",
      "Page 5 processed. Number of product links found: 16\n",
      "Page 6 processed. Number of product links found: 12\n",
      "An error occurred: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div[1]/span/div/h1/div/div[1]\"}\n",
      "  (Session info: chrome=129.0.6668.90); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF76C7BB675+29573]\n",
      "\t(No symbol) [0x00007FF76C7304A0]\n",
      "\t(No symbol) [0x00007FF76C5EB6EA]\n",
      "\t(No symbol) [0x00007FF76C63F815]\n",
      "\t(No symbol) [0x00007FF76C63FA6C]\n",
      "\t(No symbol) [0x00007FF76C68B917]\n",
      "\t(No symbol) [0x00007FF76C66733F]\n",
      "\t(No symbol) [0x00007FF76C6886BC]\n",
      "\t(No symbol) [0x00007FF76C6670A3]\n",
      "\t(No symbol) [0x00007FF76C6312DF]\n",
      "\t(No symbol) [0x00007FF76C632441]\n",
      "\tGetHandleVerifier [0x00007FF76CAEC5BD+3375821]\n",
      "\tGetHandleVerifier [0x00007FF76CB379B7+3684039]\n",
      "\tGetHandleVerifier [0x00007FF76CB2CDDB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF76C87B7F6+816390]\n",
      "\t(No symbol) [0x00007FF76C73B7AF]\n",
      "\t(No symbol) [0x00007FF76C7375D4]\n",
      "\t(No symbol) [0x00007FF76C737770]\n",
      "\t(No symbol) [0x00007FF76C7265CF]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "Final high price: 26.328125, Final low price: 26.25, Number of products: 1000\n",
      "Number of pages: 20\n",
      "Page 1 processed. Number of product links found: 0\n",
      "Page 2 processed. Number of product links found: 16\n",
      "Page 3 processed. Number of product links found: 16\n",
      "Page 4 processed. Number of product links found: 16\n",
      "Page 5 processed. Number of product links found: 16\n",
      "Page 6 processed. Number of product links found: 16\n",
      "Page 7 processed. Number of product links found: 16\n",
      "Page 8 processed. Number of product links found: 16\n",
      "Page 9 processed. Number of product links found: 16\n",
      "Page 10 processed. Number of product links found: 16\n",
      "Page 11 processed. Number of product links found: 16\n",
      "Page 12 processed. Number of product links found: 16\n",
      "Page 13 processed. Number of product links found: 16\n",
      "Page 14 processed. Number of product links found: 16\n",
      "Page 15 processed. Number of product links found: 16\n",
      "Page 16 processed. Number of product links found: 16\n",
      "Page 17 processed. Number of product links found: 16\n",
      "Page 18 processed. Number of product links found: 16\n",
      "Page 19 processed. Number of product links found: 16\n",
      "Page 20 processed. Number of product links found: 2\n"
     ]
    }
   ],
   "source": [
    "amazon_first_links3 =[]\n",
    "low_price = 80\n",
    "high_price = \"\"\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    url = f\"https://www.amazon.co.uk/s?k=dvd&i=merchant-items&me=A1UFLUL80142N4&s=price-desc-rank&page={page}&qid=1728405422&rnid=389127011&ref=sr_nr_p_36_0_0&low-price={low_price}&high-price={high_price}\"\n",
    "    amazon_driver.get(url)\n",
    "\n",
    "    # Initialize previous range variables\n",
    "    previous_low_price = low_price\n",
    "    previous_high_price = high_price\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get the number of products\n",
    "            Num_of_products = int(amazon_driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/span/div/h1/div/div[1]\").text.split(\"results\")[0].strip().split(\" \")[-1].replace(\",\", ''))\n",
    "        except Exception:\n",
    "            Num_of_products = 0\n",
    "\n",
    "        # Check if the number of products is greater than 320\n",
    "        if Num_of_products > 320:\n",
    "            try:\n",
    "                time.sleep(3)\n",
    "\n",
    "                # If low_price and high_price are the same, increment high_price slightly\n",
    "                if round(low_price, 2) == round(high_price, 2):\n",
    "                    high_price += 0.01\n",
    "                    break\n",
    "\n",
    "                # Calculate the new high price\n",
    "                new_high_price = (low_price + high_price) / 2\n",
    "\n",
    "                # Ensure new_high_price is still greater than low_price\n",
    "                if new_high_price <= low_price:\n",
    "                    new_high_price = low_price + 0.01  # Adjust if needed\n",
    "\n",
    "                # Build the updated URL with the new high_price\n",
    "                url = f\"https://www.amazon.co.uk/s?k=dvd&i=merchant-items&me=A1UFLUL80142N4&s=price-desc-rank&page={page}&qid=1728405422&rnid=389127011&ref=sr_nr_p_36_0_0&low-price={low_price}&high-price={new_high_price}\"\n",
    "\n",
    "                # Navigate to the updated URL\n",
    "                amazon_driver.get(url)\n",
    "\n",
    "                # Re-fetch the number of products\n",
    "                Num_of_products = int(amazon_driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/span/div/h1/div/div[1]\").text.split(\"results\")[0].strip().split(\" \")[-1].replace(\",\", ''))\n",
    "\n",
    "                # If no products found, revert to the previous range\n",
    "                if Num_of_products == 0:\n",
    "                    low_price = previous_low_price\n",
    "                    high_price = previous_high_price\n",
    "                    print(\"No products found, reverting to previous range.\")\n",
    "                    break  # Exit the inner loop to retry with the previous range\n",
    "                else:\n",
    "                    # Update previous range for the next iteration\n",
    "                    previous_low_price = low_price\n",
    "                    previous_high_price = high_price\n",
    "                    high_price = new_high_price  # Update to the new high price\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break  # Exit the inner loop on error\n",
    "        else:\n",
    "            break  # If the number of products is 320 or less, exit the loop\n",
    "\n",
    "    print(f\"Final high price: {high_price}, Final low price: {low_price}, Number of products: {Num_of_products}\")\n",
    "\n",
    "    # Pagination handling\n",
    "    pages = 20 if Num_of_products / 16 >= 20 else math.ceil(Num_of_products / 16)\n",
    "    print(f\"Number of pages: {pages}\")\n",
    "\n",
    "    for i in range(1, pages + 1):\n",
    "        if i != 1:\n",
    "            url = f\"https://www.amazon.co.uk/s?k=dvd&i=merchant-items&me=A1UFLUL80142N4&s=price-desc-rank&page={i}&qid=1728405422&rnid=389127011&ref=sr_nr_p_36_0_0&low-price={low_price}&high-price={high_price}\"\n",
    "            amazon_driver.get(url)\n",
    "\n",
    "        # Scroll down to load more products\n",
    "        last_height = amazon_driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        tries = 20  # Number of tries before stopping\n",
    "\n",
    "        for j in range(tries):\n",
    "            try:\n",
    "                time.sleep(1)  # Wait for page to load\n",
    "                amazon_driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # Scroll to the bottom\n",
    "                new_height = amazon_driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "                if new_height == last_height:  # Check if we've reached the end\n",
    "                    break  # Exit the scrolling loop if no more content is loading\n",
    "                \n",
    "                last_height = new_height  # Update the last height\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "\n",
    "        # Parse the page content\n",
    "        html_source = amazon_driver.page_source\n",
    "        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        products_link_tags = soup.find_all(class_=\"a-link-normal s-no-outline\")\n",
    "        amazon_products_links = [\"https://www.amazon.co.uk/\" + products_link['href'] for products_link in products_link_tags]\n",
    "        \n",
    "        amazon_first_links3.extend(amazon_products_links)  # Append new links to the list\n",
    "        print(f\"Page {i} processed. Number of product links found: {len(amazon_products_links)}\")\n",
    "\n",
    "    # Prepare for the next price range\n",
    "    temp_price = low_price\n",
    "    low_price = high_price\n",
    "    #high_price += 5  # Increment the high price for the next iteration\n",
    "    break\n",
    "\n",
    "# Print or process the gathered product links\n",
    "print(f\"Total product links gathered: {len(amazon_first_links3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_output_links.txt\", \"r\") as file:\n",
    "    amazon_first_links = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "amazon_titles= []\n",
    "amazon_links = []\n",
    "webuy_titles = []\n",
    "webuy_links = []\n",
    "amazon_images = []\n",
    "webuy_images = []\n",
    "calculated_prices = []\n",
    "product_codes = []\n",
    "buy_prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "Price after reducing 1% from 30.82 is 30.5118\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "Element not found\n",
      "Element not found or not clickable via JS path: Message: javascript error: Cannot read properties of null (reading 'click')\n",
      "  (Session info: chrome=129.0.6668.90)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7376BB095+29557]\n",
      "\t(No symbol) [0x00007FF73762FA50]\n",
      "\t(No symbol) [0x00007FF7374EB56A]\n",
      "\t(No symbol) [0x00007FF7374F213E]\n",
      "\t(No symbol) [0x00007FF7374F47F4]\n",
      "\t(No symbol) [0x00007FF7375897F0]\n",
      "\t(No symbol) [0x00007FF73756718A]\n",
      "\t(No symbol) [0x00007FF73758851C]\n",
      "\t(No symbol) [0x00007FF737566F33]\n",
      "\t(No symbol) [0x00007FF73753116F]\n",
      "\t(No symbol) [0x00007FF7375322D1]\n",
      "\tGetHandleVerifier [0x00007FF7379EC96D+3378253]\n",
      "\tGetHandleVerifier [0x00007FF737A38497+3688311]\n",
      "\tGetHandleVerifier [0x00007FF737A2D1CB+3642539]\n",
      "\tGetHandleVerifier [0x00007FF73777A6B6+813462]\n",
      "\t(No symbol) [0x00007FF73763AB5F]\n",
      "\t(No symbol) [0x00007FF737636B74]\n",
      "\t(No symbol) [0x00007FF737636D10]\n",
      "\t(No symbol) [0x00007FF737625C1F]\n",
      "\tBaseThreadInitThunk [0x00007FFFEB127374+20]\n",
      "\tRtlUserThreadStart [0x00007FFFECE5CC91+33]\n",
      "\n",
      "Element not found\n",
      "elemet found\n",
      "'aria-label'\n",
      "No sellers in the list are whitelisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "Element not found\n",
      "Element not found or not clickable via JS path: Message: javascript error: Cannot read properties of null (reading 'click')\n",
      "  (Session info: chrome=129.0.6668.90)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7376BB095+29557]\n",
      "\t(No symbol) [0x00007FF73762FA50]\n",
      "\t(No symbol) [0x00007FF7374EB56A]\n",
      "\t(No symbol) [0x00007FF7374F213E]\n",
      "\t(No symbol) [0x00007FF7374F47F4]\n",
      "\t(No symbol) [0x00007FF7375897F0]\n",
      "\t(No symbol) [0x00007FF73756718A]\n",
      "\t(No symbol) [0x00007FF73758851C]\n",
      "\t(No symbol) [0x00007FF737566F33]\n",
      "\t(No symbol) [0x00007FF73753116F]\n",
      "\t(No symbol) [0x00007FF7375322D1]\n",
      "\tGetHandleVerifier [0x00007FF7379EC96D+3378253]\n",
      "\tGetHandleVerifier [0x00007FF737A38497+3688311]\n",
      "\tGetHandleVerifier [0x00007FF737A2D1CB+3642539]\n",
      "\tGetHandleVerifier [0x00007FF73777A6B6+813462]\n",
      "\t(No symbol) [0x00007FF73763AB5F]\n",
      "\t(No symbol) [0x00007FF737636B74]\n",
      "\t(No symbol) [0x00007FF737636D10]\n",
      "\t(No symbol) [0x00007FF737625C1F]\n",
      "\tBaseThreadInitThunk [0x00007FFFEB127374+20]\n",
      "\tRtlUserThreadStart [0x00007FFFECE5CC91+33]\n",
      "\n",
      "Element not found\n",
      "Element not found\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "elemet found\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "No sellers in the list are blacklisted.\n",
      "Price after reducing 1% from 30.82 is 30.5118\n",
      "cookies found\n",
      "Similarity with 'Little Buddha': 81.25%\n",
      "Libraries are installed correctly!\n",
      "Similarity with 'images\\temporary_images\\5060034576228_l.jpg': 47.00%\n",
      "Deleted: images\\temporary_images\\044007433768_l.jpg\n",
      "Deleted: images\\temporary_images\\5018755256011_l.jpg\n",
      "Deleted: images\\temporary_images\\5023363019323_l.jpg\n",
      "Deleted: images\\temporary_images\\5034504936577_l.jpg\n",
      "Deleted: images\\temporary_images\\5060034576228_l.jpg\n",
      "Deleted: images\\temporary_images\\5060105723377_l.jpg\n",
      "Deleted: images\\temporary_images\\807839005059_l.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for k in range(5001, 6000):\n",
    "    link = amazon_first_links[k]\n",
    "    valid_product = True\n",
    "    while valid_product == True:\n",
    "        amazon_driver.get(link)\n",
    "\n",
    "        product_code = link.split(\"/dp/\")[1].split(\"/\")[0] \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        other_sellers_button1 = '/html/body/div[2]/div/div[5]/div[4]/div[1]/div[5]/div[2]/div/div[2]/a/span'\n",
    "        other_sellers_button2 = 'buybox-see-all-buying-choices'\n",
    "        other_sellers_button3 = \"/html/body/div[3]/div/div[8]/div[4]/div[1]/div[5]/div[2]/div/div[2]/a/span/span[1]\"\n",
    "        other_sellers_button4 = \"a-icon a-icon-arrow a-icon-small daodi-arrow-icon\"\n",
    "        section_opened = False\n",
    "        other_sellers_button_PATH = 'a-price'\n",
    "\n",
    "        outofstock = False\n",
    "        for i in range(0,3):\n",
    "            try: \n",
    "                html_source = amazon_driver.page_source\n",
    "                soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                check_stock = soup.find(id=\"outOfStock\")\n",
    "                if check_stock:\n",
    "                    outofstock = True\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        if outofstock:\n",
    "            print(\"out\")\n",
    "            valid_product = False\n",
    "            pass\n",
    "\n",
    "        element_found = False\n",
    "        try:\n",
    "            # Wait until the element is clickable\n",
    "            WebDriverWait(amazon_driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, other_sellers_button1))\n",
    "            )\n",
    "            amazon_driver.find_element(By.XPATH, other_sellers_button1).click()\n",
    "            print(\"elemet found\")\n",
    "            element_found = True\n",
    "            time.sleep(1)\n",
    "            for i in range(0,3):\n",
    "                    try: \n",
    "                        html_source = amazon_driver.page_source\n",
    "                        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                        slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "                        if slider:\n",
    "                            section_opened = True\n",
    "                            break\n",
    "                        else:\n",
    "                            time.sleep(1)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        except Exception as e:\n",
    "            element_found = False\n",
    "            print(f\"Element not found\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        if not element_found:\n",
    "            other_sellers_button_js_path = 'document.querySelector(\"#dynamic-aod-ingress-box > div > div.a-section.a-spacing-none.daodi-content > a > span > span:nth-child(1)\")'\n",
    "\n",
    "            try:\n",
    "                # Use JavaScript to find and click the element\n",
    "                amazon_driver.execute_script(f'{other_sellers_button_js_path}.click();')\n",
    "                print(\"Element clicked via JS path\")\n",
    "                element_found = True\n",
    "                time.sleep(1)\n",
    "                \n",
    "                for i in range(3):\n",
    "                    try:\n",
    "                        html_source = amazon_driver.page_source\n",
    "                        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                        slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "                        if slider:\n",
    "                            section_opened = True\n",
    "                            break\n",
    "                        else:\n",
    "                            time.sleep(1)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            except Exception as e:\n",
    "                element_found = False\n",
    "                print(f\"Element not found or not clickable via JS path: {e}\")\n",
    "                time.sleep(0.5)\n",
    "\n",
    "\n",
    "        if not element_found:\n",
    "            try:\n",
    "                # Wait until the element is clickable\n",
    "                WebDriverWait(amazon_driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, other_sellers_button3))\n",
    "                )\n",
    "                amazon_driver.find_element(By.XPATH, other_sellers_button3).click()\n",
    "                print(\"elemet found\")\n",
    "                element_found = True\n",
    "                time.sleep(1)\n",
    "                for i in range(0,3):\n",
    "                        try: \n",
    "                            html_source = amazon_driver.page_source\n",
    "                            soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                            slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "                            if slider:\n",
    "                                section_opened = True\n",
    "                                break\n",
    "                            else:\n",
    "                                time.sleep(1)\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "            except Exception as e:\n",
    "                element_found = False\n",
    "                print(f\"Element not found\")\n",
    "                time.sleep(0.5)\n",
    "        if not section_opened:\n",
    "            try:\n",
    "                    \n",
    "                # Wait until the element is clickable\n",
    "                WebDriverWait(amazon_driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.ID, other_sellers_button2))\n",
    "                )\n",
    "                amazon_driver.find_element(By.ID, other_sellers_button2).click()\n",
    "                print(\"elemet found\")\n",
    "                time.sleep(1)\n",
    "                section_opened = True\n",
    "            except Exception:\n",
    "                section_opened = False\n",
    "                print(\"Element not found\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if section_opened:\n",
    "            for i in range(0,3):\n",
    "                    try: \n",
    "                        html_source = amazon_driver.page_source\n",
    "                        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                        slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "                        if slider:\n",
    "                            section_opened = True\n",
    "                            break\n",
    "                        else:\n",
    "                            time.sleep(1)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "        else:\n",
    "\n",
    "            html_source = amazon_driver.page_source\n",
    "            soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            amazon_sellertag = soup.find(id=\"sellerProfileTriggerId\")\n",
    "            amazon_pricetag = soup.find(class_=\"a-size-base a-color-price offer-price a-text-normal\")\n",
    "            if amazon_sellertag and amazon_pricetag:\n",
    "                amazon_price = float(amazon_pricetag.text.strip().replace(\"£\", \"\"))\n",
    "\n",
    "                if amazon_sellertag.text not in seller_white_list:\n",
    "                    valid_product = False\n",
    "                    continue\n",
    "            else:\n",
    "                valid_product = False\n",
    "                continue\n",
    "    \n",
    "\n",
    "        if section_opened:\n",
    "            for i in range(0,5):\n",
    "                try: \n",
    "                    html_source = amazon_driver.page_source\n",
    "                    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                    slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "                    if slider:\n",
    "                        amazon_sellerstags = slider.find_all(class_=\"a-size-small a-link-normal\")\n",
    "                        amazon_pricestags = slider.find_all(class_=\"a-section a-spacing-none aok-align-center aok-relative\")\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(1)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "\n",
    "\n",
    "            amazon_sellers = []\n",
    "            for seller in amazon_sellerstags:\n",
    "                try:\n",
    "                    # Check if seller is valid and has the required aria-label attribute\n",
    "                    if seller[\"aria-label\"] == \"Opens a new page\":\n",
    "                        amazon_sellers.append(seller.text)  # Add to the list if conditions are met\n",
    "                except Exception as e:\n",
    "                    # Handle any exceptions (e.g., element doesn't support get_attribute)\n",
    "                    print(e)  # You can log the exception if needed or just pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Example usage\n",
    "            Wseller_index = check_whitelisted_sellers(amazon_sellers, seller_white_list)\n",
    "            if Wseller_index == -1:\n",
    "                valid_product = False\n",
    "                continue\n",
    "            if check_blacklisted_sellers(amazon_sellers, sellers_black_list):\n",
    "                valid_product = False\n",
    "                continue\n",
    "            sellers20pounds_counter = 0\n",
    "            sellers10pounds_counter = 0\n",
    "\n",
    "            amazon_prices = []\n",
    "            for p in amazon_pricestags:\n",
    "                priceSTR = p.find(class_='aok-offscreen').text.strip().split(\" \")[0].replace(\"£\", \"\")\n",
    "                riceNMR = float(priceSTR.replace(\"£\", \"\"))\n",
    "                amazon_prices.append(riceNMR)\n",
    "                if riceNMR <= 20.00:\n",
    "                    sellers20pounds_counter += 1\n",
    "                if riceNMR <= 10.00:\n",
    "                    sellers10pounds_counter += 1\n",
    "            if sellers20pounds_counter > 2 or sellers10pounds_counter >= 2:\n",
    "                valid_product = False\n",
    "                continue\n",
    "                print(\"Not a valid product\")\n",
    "            amazon_price = amazon_prices [Wseller_index]\n",
    "        \n",
    "\n",
    "\n",
    "        percentage = 0.01\n",
    "        # Calculate 1% of the price\n",
    "        reduction = amazon_price * percentage\n",
    "\n",
    "        # Calculate the new price\n",
    "        calculated_price = amazon_price - reduction\n",
    "\n",
    "        \n",
    "        print(\"Price after reducing 1% from\", amazon_price, \"is\", calculated_price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # Removing duplicates while maintaining order using a set\n",
    "        # unique_sellers = []\n",
    "        # seen_sellers = set()\n",
    "        # for seller in sellers:\n",
    "        #     if seller not in seen_sellers:\n",
    "        #         unique_sellers.append(seller)\n",
    "        #         seen_sellers.add(seller)\n",
    "\n",
    "        # unique_prices = []\n",
    "        # seen_prices = set()\n",
    "        # for price in prices:\n",
    "        #     if price not in seen_prices:\n",
    "        #         unique_prices.append(price)\n",
    "        #         seen_prices.add(price)\n",
    "\n",
    "        # print(unique_sellers)  # This will maintain the original order of sellers\n",
    "        # print(unique_prices)   # This will maintain the original order of prices\n",
    "\n",
    "        html_source = amazon_driver.page_source\n",
    "        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        product_title = soup.find(id=\"productTitle\").text.strip()\n",
    "        if check_black_list_words(words_black_list, product_title):\n",
    "            valid_product = False\n",
    "            continue\n",
    "        \n",
    "        product_image_tag = soup.find(id='imgTagWrapperId')\n",
    "        product_image = product_image_tag.find('img')['src']\n",
    "        \n",
    "\n",
    "        webuy_url = f\"https://uk.webuy.com/search?stext={product_title.replace(' ', '+').replace('&', 'and')}\"\n",
    "        webuy_driver.get(webuy_url)\n",
    "        time.sleep(5)\n",
    "        html_source = webuy_driver.page_source\n",
    "        webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        for _ in range(0, 5):\n",
    "            html_source = webuy_driver.page_source\n",
    "            webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "\n",
    "            cookies = webuy_soup.find(id=\"onetrust-accept-btn-handler\")\n",
    "            cookies_path = '/html/body/div[3]/div[2]/div/div/div[2]/div/div/button'\n",
    "            if cookies:\n",
    "                print(\"cookies found\")\n",
    "                wait_for_element_to_be_clickable(webuy_driver, cookies_path).click()\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        for _ in range(0, 5):\n",
    "\n",
    "\n",
    "            html_source = webuy_driver.page_source\n",
    "            webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            results = webuy_soup.find(class_='search-result-grid mb-s md-mb-0')\n",
    "            if results:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        try:\n",
    "            html_source = webuy_driver.page_source\n",
    "            webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            results = webuy_soup.find(class_='search-result-grid mb-s md-mb-0')\n",
    "        except Exception:\n",
    "            valid_product = False\n",
    "            continue\n",
    "\n",
    "        webuy_products = results.find_all(class_='search-product-card')\n",
    "\n",
    "        webuy_product_titles = []\n",
    "        webuy_product_links = []\n",
    "        webuy_product_images = []\n",
    "        for product in webuy_products:\n",
    "            webuy_product_titles.append(product.find(class_='card-title').text)\n",
    "            webuy_product_links.append('https://uk.webuy.com' + product.find('a')['href'])\n",
    "            webuy_product_images.append(product.find('a').find(\"img\")['src'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        title_similarity_results = calculate_similarity(product_title, webuy_product_titles)\n",
    "\n",
    "        # Print results\n",
    "        for string, score in title_similarity_results:\n",
    "            print(f\"Similarity with '{string}': {score:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Libraries are installed correctly!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Download images\n",
    "        downloaded_images = download_images(webuy_product_images)\n",
    "\n",
    "        # Download the reference image\n",
    "        reference_image_path = download_reference_image(product_image)\n",
    "        \n",
    "        # Compare each downloaded image with the reference image if it was successfully downloaded\n",
    "        if reference_image_path:\n",
    "            image_similarity_results = []\n",
    "            for img_path in downloaded_images:\n",
    "                similarity = compare_images(reference_image_path, img_path)\n",
    "                image_similarity_results.append((img_path, similarity))\n",
    "\n",
    "            # Print results\n",
    "            for img_path, score in image_similarity_results:\n",
    "                print(f\"Similarity with '{img_path}': {score:.2f}%\")\n",
    "        else:\n",
    "            print(\"Reference image download failed; comparison will not be performed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate percentages\n",
    "        percentages = calculate_percentages(title_similarity_results, image_similarity_results)\n",
    "\n",
    "\n",
    "        # Finding the highest percentage and its index\n",
    "        highest_percentage = max(percentages)\n",
    "        highest_index = percentages.index(highest_percentage)\n",
    "\n",
    "        webuy_link = webuy_product_links[highest_index]\n",
    "        webuy_driver.get(webuy_link)\n",
    "        \n",
    "\n",
    "\n",
    "        # Example usage\n",
    "        clear_temp_images()\n",
    "        time.sleep(2)\n",
    "        for _ in range(0, 5):\n",
    "            html_source = webuy_driver.page_source\n",
    "            webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "            cookies = webuy_soup.find(id=\"onetrust-accept-btn-handler\")\n",
    "            cookies_path = '/html/body/div[3]/div[2]/div/div/div[2]/div/div/button'\n",
    "            if cookies:\n",
    "                print(\"cookies found\")\n",
    "                wait_for_element_to_be_clickable(webuy_driver, cookies_path).click()\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        for _ in range(0, 5):\n",
    "            html_source = webuy_driver.page_source\n",
    "            webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            # Extracting the price\n",
    "            if webuy_soup.find(class_=\"sell-price md-mb-0 d-block w-100\") and webuy_soup.find(class_=\"heading-s-semibold md-heading-l-semibold\")and webuy_soup.find(class_=\"product-gallery-image t058-checked\"):\n",
    "                webuy_priceSTR = webuy_soup.find(class_=\"sell-price md-mb-0 d-block w-100\").text.replace(\"£\", \"\")\n",
    "                webuy_title = webuy_soup.find(class_=\"heading-s-semibold md-heading-l-semibold\").text\n",
    "                webuy_image_link = webuy_soup.find(class_=\"product-gallery-image t058-checked\").find(\"img\")['src']\n",
    "\n",
    "                break        \n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        webuy_image = download_webuy_image(webuy_image_link)\n",
    "        # Converting the price to a float and adding 3\n",
    "        webuy_price = float(webuy_priceSTR) + 3\n",
    "        formatted_price = f\"£{webuy_price:.2f}\"\n",
    "\n",
    "        \n",
    "        if check_prices_more_than_webuyPrice(amazon_prices, (webuy_price*2)):\n",
    "            valid_product = False\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Alternatively, using an f-string\n",
    "\n",
    "        # Printing the formatted price\n",
    "        print(\"The formatted price is:\", formatted_price)\n",
    "        for _ in range(0, 5):\n",
    "            check_store_button_PATH = '/html/body/div[1]/div/main/div/div/div[1]/div[1]/div[2]/div[4]/div[2]/div/span'\n",
    "\n",
    "            try:\n",
    "                # Wait until the element is clickable\n",
    "                WebDriverWait(webuy_driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, check_store_button_PATH))\n",
    "                )\n",
    "                webuy_driver.find_element(By.XPATH, check_store_button_PATH).click()\n",
    "            except Exception as e:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        for _ in range(0, 5):\n",
    "            \n",
    "            all_store_button_PATH = '/html/body/div[1]/div/main/div/div/div[6]/div[2]/button[2]/span'\n",
    "\n",
    "            try:\n",
    "                # Wait until the element is clickable\n",
    "                WebDriverWait(webuy_driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, all_store_button_PATH))\n",
    "                )\n",
    "                webuy_driver.find_element(By.XPATH, all_store_button_PATH).click()\n",
    "            except Exception as e:\n",
    "                time.sleep(0.5)\n",
    "        html_source = webuy_driver.page_source\n",
    "        webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        store_name_class = 'store-name d-flex justify-content-between'\n",
    "        for _ in range(0, 5):\n",
    "            html_source = webuy_driver.page_source\n",
    "            webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            webuy_sellers_tag = webuy_soup.find(class_=\"store-list\")\n",
    "            if webuy_sellers_tag:\n",
    "                stores = webuy_sellers_tag.find_all(class_=\"store-card\")\n",
    "                check_store_button_present = True\n",
    "            else:\n",
    "                check_store_button_present = False\n",
    "            \n",
    "        if check_store_button_present == False or len(stores) < 3:\n",
    "            valid_product = False\n",
    "            continue\n",
    "        file_path = \"storage.json\"\n",
    "        # Initial new data values (replace these with actual data in your script)\n",
    "        new_data = {\n",
    "            'amazon_title': product_title,\n",
    "            'amazon_link': link,\n",
    "            'webuy_title': webuy_title,\n",
    "            'webuy_link': webuy_link,\n",
    "            'amazon_image': reference_image_path,\n",
    "            'webuy_image': webuy_image,\n",
    "            'calculated_price': calculated_price,\n",
    "            'product_code': product_code,\n",
    "            'webuy_price': float(f\"{webuy_price:.2f}\")\n",
    "        }\n",
    "\n",
    "        # Function to load the existing data from the JSON file (or create new lists if file doesn't exist)\n",
    "        def load_existing_data(file_path):\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    try:\n",
    "                        return json.load(file)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # If the file is empty or invalid JSON, start with empty lists\n",
    "                        return {\n",
    "                            'amazon_titles': [],\n",
    "                            'amazon_links': [],\n",
    "                            'webuy_titles': [],\n",
    "                            'webuy_links': [],\n",
    "                            'amazon_images': [],\n",
    "                            'webuy_images': [],\n",
    "                            'calculated_prices': [],\n",
    "                            'product_codes': [],\n",
    "                            'webuy_prices': []\n",
    "                        }\n",
    "            else:\n",
    "                # If the file doesn't exist, return empty lists\n",
    "                return {\n",
    "                    'amazon_titles': [],\n",
    "                    'amazon_links': [],\n",
    "                    'webuy_titles': [],\n",
    "                    'webuy_links': [],\n",
    "                    'amazon_images': [],\n",
    "                    'webuy_images': [],\n",
    "                    'calculated_prices': [],\n",
    "                    'product_codes': [],\n",
    "                    'webuy_prices': []\n",
    "                }\n",
    "\n",
    "        # Load the existing data\n",
    "        existing_data = load_existing_data(file_path)\n",
    "\n",
    "        # Check the conditions and append new data to each corresponding list\n",
    "        if all([\n",
    "            new_data['amazon_title'] is not None,\n",
    "            new_data['amazon_link'] is not None,\n",
    "            new_data['webuy_title'] is not None,\n",
    "            new_data['webuy_link'] is not None,\n",
    "            new_data['amazon_image'] is not None,\n",
    "            new_data['webuy_image'] is not None,\n",
    "            new_data['calculated_price'] is not None,\n",
    "            new_data['product_code'] is not None,\n",
    "            new_data['webuy_price'] is not None\n",
    "        ]):\n",
    "            existing_data['amazon_titles'].append(new_data['amazon_title'])\n",
    "            existing_data['amazon_links'].append(new_data['amazon_link'])\n",
    "            existing_data['webuy_titles'].append(new_data['webuy_title'])\n",
    "            existing_data['webuy_links'].append(new_data['webuy_link'])\n",
    "            existing_data['amazon_images'].append(new_data['amazon_image'])\n",
    "            existing_data['webuy_images'].append(new_data['webuy_image'])\n",
    "            existing_data['calculated_prices'].append(new_data['calculated_price'])\n",
    "            existing_data['product_codes'].append(new_data['product_code'])\n",
    "            existing_data['webuy_prices'].append(new_data['webuy_price'])\n",
    "\n",
    "            # Save updated data back to the JSON file\n",
    "            with open(file_path, 'w') as file:\n",
    "                json.dump(existing_data, file, indent=4)\n",
    "\n",
    "            print(f\"Data successfully added to {file_path}!\")\n",
    "\n",
    "        else:\n",
    "            valid_product = False\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amazon_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mamazon_titles\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'amazon_titles' is not defined"
     ]
    }
   ],
   "source": [
    "len(amazon_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read the data back from the file\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# New variables for the lists to avoid overwriting the original ones\n",
    "new_amazon_titles = data['Amazon_title']\n",
    "new_amazon_links = data['Amazon_link']\n",
    "new_webuy_titles = data['Webuy_title']\n",
    "new_webuy_links = data['Webuy_link']\n",
    "new_amazon_images = data['Amazon_image']\n",
    "new_webuy_images = data['Webuy_image']\n",
    "new_calculated_prices = data['Calculated_price']\n",
    "new_product_codes = data['Product_code']\n",
    "new_buy_prices = data['Buy_price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your data\n",
    "data = {\n",
    "    'Amazon_title': new_amazon_titles,\n",
    "    'Amazon_link': new_amazon_links,\n",
    "    'Webuy_title': new_webuy_titles,\n",
    "    'Webuy_link': new_webuy_links,\n",
    "    'Amazon_image': new_amazon_images,\n",
    "    'Webuy_image': new_webuy_images,\n",
    "    'Calculated_price': new_calculated_prices,\n",
    "    'Buy_price': new_buy_prices,\n",
    "    'Product_code': new_product_codes,\n",
    "}\n",
    "\n",
    "# Create a new workbook and select the active worksheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Products\"\n",
    "\n",
    "# Write headers in the specified order\n",
    "headers = ['Amazon Title', 'Amazon Link', 'Webuy Title', 'Amazon Image', 'Webuy Image', 'Calculated Price', 'Product Code', 'Buy Price', 'Webuy Link']\n",
    "ws.append(headers)\n",
    "\n",
    "# Define styles for headers\n",
    "header_fill = PatternFill(start_color=\"808080\", end_color=\"808080\", fill_type=\"solid\")  # Gray fill\n",
    "header_font = Font(bold=True)  # Bold font\n",
    "\n",
    "# Apply styles to headers and align vertically\n",
    "for col_num, header in enumerate(headers, start=1):\n",
    "    cell = ws.cell(row=1, column=col_num)\n",
    "    cell.fill = header_fill\n",
    "    cell.font = header_font\n",
    "    cell.alignment = Alignment(vertical='center')  # Vertically align headers\n",
    "\n",
    "# Set column widths\n",
    "column_widths = [30, 20, 30, 15, 15, 15, 15, 15, 35]  # Adjust widths as needed\n",
    "for col_num, width in enumerate(column_widths, start=1):\n",
    "    ws.column_dimensions[openpyxl.utils.get_column_letter(col_num)].width = width\n",
    "\n",
    "# Add data and images\n",
    "for i in range(len(data['Amazon_title'])):\n",
    "    # Amazon Title\n",
    "    amazon_title_cell = ws.cell(row=i + 2, column=1, value=data['Amazon_title'][i])\n",
    "    amazon_title_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "\n",
    "    # Amazon Link (displays \"Amazon Link\" as plain text but clickable)\n",
    "    amazon_link_cell = ws.cell(row=i + 2, column=2, value=\"Amazon Link\")\n",
    "    amazon_link_cell.hyperlink = data['Amazon_link'][i]  # Set the hyperlink\n",
    "    amazon_link_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "\n",
    "    # Webuy Title\n",
    "    webuy_title_cell = ws.cell(row=i + 2, column=3, value=data['Webuy_title'][i])\n",
    "    webuy_title_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "\n",
    "    # Amazon Image in column 'D'\n",
    "    amazon_img_path = data['Amazon_image'][i]\n",
    "    amazon_img = Image(amazon_img_path)\n",
    "    amazon_img.width = 50\n",
    "    amazon_img.height = 50\n",
    "    amazon_img.anchor = f'D{i + 2}'  # Set image in column D\n",
    "    ws.add_image(amazon_img)\n",
    "\n",
    "    # Webuy Image in column 'E'\n",
    "    webuy_img_path = data['Webuy_image'][i]\n",
    "    webuy_img = Image(webuy_img_path)\n",
    "    webuy_img.width = 50\n",
    "    webuy_img.height = 50\n",
    "    webuy_img.anchor = f'E{i + 2}'  # Set image in column E\n",
    "    ws.add_image(webuy_img)\n",
    "\n",
    "    # Calculated Price\n",
    "    calculated_price_cell = ws.cell(row=i + 2, column=6, value=f\"£{data['Calculated_price'][i]:.2f}\")\n",
    "    calculated_price_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "\n",
    "    # Product Code\n",
    "    product_code_cell = ws.cell(row=i + 2, column=7, value=data['Product_code'][i])\n",
    "    product_code_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "\n",
    "    # Buy Price\n",
    "    buy_price_cell = ws.cell(row=i + 2, column=8, value=f\"£{data['Buy_price'][i]:.2f}\")\n",
    "    buy_price_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "\n",
    "    # Webuy Link (display as regular text, clickable, and copyable)\n",
    "    webuy_link_cell = ws.cell(row=i + 2, column=9, value=data['Webuy_link'][i])\n",
    "    webuy_link_cell.hyperlink = data['Webuy_link'][i]  # Hyperlink added\n",
    "    webuy_link_cell.alignment = Alignment(vertical='center')  # Vertically align\n",
    "    webuy_link_cell.font = Font(color=\"000000\", underline=None)  # Regular black text without underline\n",
    "\n",
    "    # Set row height to fit images\n",
    "    ws.row_dimensions[i + 2].height = 50  # Ensure row height is set based on image size\n",
    "\n",
    "# Save the workbook\n",
    "wb.save('final_sheet_with_correct_order.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Data from the subsequent scripts\n",
    "new_data = {\n",
    "    'Amazon_title': amazon_titles,\n",
    "    'Amazon_link': amazon_links,\n",
    "    'Webuy_title': webuy_titles,\n",
    "    'Webuy_link': webuy_links,\n",
    "    'Amazon_image': amazon_images,\n",
    "    'Webuy_image': webuy_images,\n",
    "    'Calculated_price': calculated_prices,\n",
    "    'Product_code': product_codes,\n",
    "    'Buy_price': buy_prices,\n",
    "}\n",
    "\n",
    "# Open the existing JSON file and load the data\n",
    "with open('data.json', 'r') as json_file:\n",
    "    existing_data = json.load(json_file)\n",
    "\n",
    "# Combine the existing data with the new data\n",
    "for key in new_data:\n",
    "    if key in existing_data:\n",
    "        existing_data[key].extend(new_data[key])  # Append new data to the existing data\n",
    "\n",
    "# Write the updated data back to the JSON file\n",
    "with open('data.json', 'w') as json_file:\n",
    "    json.dump(existing_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_driver.get(\"https://www.amazon.co.uk/Anniversary-Ultimate-Collectors-Steelbook-Blu-ray/dp/B0CKJ47V2C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element not found\n",
      "Element not found\n",
      "out of stock\n"
     ]
    }
   ],
   "source": [
    "other_sellers_button1 = '/html/body/div[2]/div/div[5]/div[4]/div[1]/div[5]/div[2]/div/div[2]/a/span'\n",
    "other_sellers_button2 = 'buybox-see-all-buying-choices'\n",
    "\n",
    "section_opened = False\n",
    "other_sellers_button_PATH = 'a-price'\n",
    "try:\n",
    "    # Wait until the element is clickable\n",
    "    WebDriverWait(amazon_driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, other_sellers_button1))\n",
    "    )\n",
    "    amazon_driver.find_element(By.XPATH, other_sellers_button1).click()\n",
    "    time.sleep(1)\n",
    "    for i in range(0,3):\n",
    "            try: \n",
    "                html_source = amazon_driver.page_source\n",
    "                soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "                if slider:\n",
    "                    section_opened = True\n",
    "                    break\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "except Exception as e:\n",
    "    print(f\"Element not found\")\n",
    "    time.sleep(0.5)\n",
    "if not section_opened:\n",
    "    try:\n",
    "            \n",
    "        # Wait until the element is clickable\n",
    "        WebDriverWait(amazon_driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, other_sellers_button2))\n",
    "        )\n",
    "        amazon_driver.find_element(By.ID, other_sellers_button2).click()\n",
    "        time.sleep(1)\n",
    "        section_opened = True\n",
    "    except Exception:\n",
    "         section_opened = False\n",
    "         print(\"Element not found\")\n",
    "\n",
    "if not section_opened:\n",
    "    outofstock = False\n",
    "    for i in range(0,5):\n",
    "        try: \n",
    "            html_source = amazon_driver.page_source\n",
    "            soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            check_stock = soup.find(id=\"outOfStock\")\n",
    "            if check_stock:\n",
    "                outofstock = True\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "if outofstock:\n",
    "    valid_product = False\n",
    "    continue\n",
    "if section_opened:\n",
    "for i in range(0,3):\n",
    "        try: \n",
    "            html_source = amazon_driver.page_source\n",
    "            soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "            if slider:\n",
    "                section_opened = True\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8366"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amazon_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html_source = amazon_driver.page_source\n",
    "soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "box = soup.find(class_=\"a-box\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_sellers = []\n",
    "amazon_sellerstags = box.find(class_=\"a-section a-spacing-base\")\n",
    "amazon_pricestags = box.find(class_=\"a-size-base a-color-price offer-price a-text-normal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(amazon_sellerstags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for seller in amazon_sellerstags:\n",
    "    try:\n",
    "        # Check if seller is valid and has the required aria-label attribute\n",
    "        if seller[\"aria-label\"] == \"Opens a new page\":\n",
    "            amazon_sellers.append(seller.text)  # Add to the list if conditions are met\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions (e.g., element doesn't support get_attribute)\n",
    "        print(e)  # You can log the exception if needed or just pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
