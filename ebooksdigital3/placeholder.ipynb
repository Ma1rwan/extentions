{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6999EB645+29573]\n",
      "\t(No symbol) [0x00007FF699960470]\n",
      "\t(No symbol) [0x00007FF69981B6EA]\n",
      "\t(No symbol) [0x00007FF69986F815]\n",
      "\t(No symbol) [0x00007FF69986FA6C]\n",
      "\t(No symbol) [0x00007FF6998BB917]\n",
      "\t(No symbol) [0x00007FF69989733F]\n",
      "\t(No symbol) [0x00007FF6998B86BC]\n",
      "\t(No symbol) [0x00007FF6998970A3]\n",
      "\t(No symbol) [0x00007FF6998612DF]\n",
      "\t(No symbol) [0x00007FF699862441]\n",
      "\tGetHandleVerifier [0x00007FF699D1C58D+3375821]\n",
      "\tGetHandleVerifier [0x00007FF699D67987+3684039]\n",
      "\tGetHandleVerifier [0x00007FF699D5CDAB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF699AAB7C6+816390]\n",
      "\t(No symbol) [0x00007FF69996B77F]\n",
      "\t(No symbol) [0x00007FF6999675A4]\n",
      "\t(No symbol) [0x00007FF699967740]\n",
      "\t(No symbol) [0x00007FF69995659F]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "2000\n",
      "20\n",
      " £30.98\n",
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6999EB645+29573]\n",
      "\t(No symbol) [0x00007FF699960470]\n",
      "\t(No symbol) [0x00007FF69981B6EA]\n",
      "\t(No symbol) [0x00007FF69986F815]\n",
      "\t(No symbol) [0x00007FF69986FA6C]\n",
      "\t(No symbol) [0x00007FF6998BB917]\n",
      "\t(No symbol) [0x00007FF69989733F]\n",
      "\t(No symbol) [0x00007FF6998B86BC]\n",
      "\t(No symbol) [0x00007FF6998970A3]\n",
      "\t(No symbol) [0x00007FF6998612DF]\n",
      "\t(No symbol) [0x00007FF699862441]\n",
      "\tGetHandleVerifier [0x00007FF699D1C58D+3375821]\n",
      "\tGetHandleVerifier [0x00007FF699D67987+3684039]\n",
      "\tGetHandleVerifier [0x00007FF699D5CDAB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF699AAB7C6+816390]\n",
      "\t(No symbol) [0x00007FF69996B77F]\n",
      "\t(No symbol) [0x00007FF6999675A4]\n",
      "\t(No symbol) [0x00007FF699967740]\n",
      "\t(No symbol) [0x00007FF69995659F]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "'aria-label'\n",
      "First whitelisted seller found at index: 0\n",
      "Not a valid product\n",
      "Price after reducing 1% from 30.98 is 30.6702\n",
      "cookies found\n",
      "Similarity with 'Spider-Man 3 Film Collection (1-3) (12) 4K UHD+BR 6 Discs': 44.25%\n",
      "Similarity with 'Jurassic Park Collection - 3 Films (PG) 3D+BR': 35.64%\n",
      "Similarity with 'Predator Trilogy 3 Film Collection (18) 3 Disc': 45.10%\n",
      "Similarity with 'Fantastic Beasts: 3-Film Collection (12) 3 Disc': 44.66%\n",
      "Similarity with 'Heroes & Villains (18) 3 Films collection (3 Disc)': 43.40%\n",
      "Similarity with 'Fascination Coral Reef 3D -3-Film Collection': 42.00%\n",
      "Similarity with 'Fantastic Beasts 2 Film Collection (12) 3 Discs': 44.66%\n",
      "Similarity with 'Very Best Of Laurel & Hardy, The : 5 Film Collection (U) 3 Disc': 40.34%\n",
      "Similarity with 'Creed: Film Collection 1-3 (12) 3 Disc': 48.94%\n",
      "Similarity with 'Three Musketeers, The: 2 Film Collection - D'Artagnan/Milady (15) 2 Discs': 44.96%\n",
      "Similarity with 'Otto Preminger Film Noir Collection LE (12) 3 Disc': 43.40%\n",
      "Similarity with 'Taken 3 (15) 2014': 32.88%\n",
      "Similarity with 'Taken 3 (15) 2014 Limited Ed. Steelbook': 35.79%\n",
      "Similarity with 'Taken Trilogy (18) 3 Disc': 27.16%\n",
      "Similarity with 'Tremors: Ultimate Collection (15) (7 BR Discs+ 3 DVD Discs)': 33.04%\n",
      "Similarity with 'Kraftwerk 3-D The Catalogue (E) 2017 BR+DVD': 34.34%\n",
      "Similarity with 'Iron Man 1-3 Complete Collection (12) 4K UHD+BR': 38.83%\n",
      "Libraries are installed correctly!\n",
      "Similarity with 'images\\temporary_images\\5050630688531_l.jpg': -4.91%\n",
      "Similarity with 'images\\temporary_images\\5053083039868_l.jpg': -1.61%\n",
      "Similarity with 'images\\temporary_images\\5039036050197_l.jpg': -9.37%\n",
      "Similarity with 'images\\temporary_images\\5051892237970_l.jpg': 17.61%\n",
      "Similarity with 'images\\temporary_images\\5060000705089_l.jpg': -0.25%\n",
      "Similarity with 'images\\temporary_images\\5050582936292_l.jpg': 15.20%\n",
      "Similarity with 'images\\temporary_images\\5051892221351_l.jpg': -9.37%\n",
      "Similarity with 'images\\temporary_images\\5053083175498_l.jpg': 9.92%\n",
      "Similarity with 'images\\temporary_images\\5051892242615_l.jpg': 3.87%\n",
      "Similarity with 'images\\temporary_images\\5017239153136_l.jpg': -0.73%\n",
      "Similarity with 'images\\temporary_images\\5035673012161_l.jpg': -10.01%\n",
      "Similarity with 'images\\temporary_images\\5039036072786_l.jpg': -6.12%\n",
      "Similarity with 'images\\temporary_images\\5039036073301_l.jpg': 1.85%\n",
      "Similarity with 'images\\temporary_images\\5039036072908_l.jpg': 35.00%\n",
      "Similarity with 'images\\temporary_images\\5053083266615_m.jpg': 33.52%\n",
      "Similarity with 'images\\temporary_images\\190295924973_l.jpg': -9.37%\n",
      "Similarity with 'images\\temporary_images\\8717418552565_l.jpg': 14.04%\n",
      "Deleted: images\\temporary_images\\190295924973_l.jpg\n",
      "Deleted: images\\temporary_images\\5017239153136_l.jpg\n",
      "Deleted: images\\temporary_images\\5035673012161_l.jpg\n",
      "Deleted: images\\temporary_images\\5039036050197_l.jpg\n",
      "Deleted: images\\temporary_images\\5039036072786_l.jpg\n",
      "Deleted: images\\temporary_images\\5039036073301_l.jpg\n",
      "Deleted: images\\temporary_images\\5050582936292_l.jpg\n",
      "Deleted: images\\temporary_images\\5050630688531_l.jpg\n",
      "Deleted: images\\temporary_images\\5051892221351_l.jpg\n",
      "Deleted: images\\temporary_images\\5051892237970_l.jpg\n",
      "Deleted: images\\temporary_images\\5051892242615_l.jpg\n",
      "Deleted: images\\temporary_images\\5053083039868_l.jpg\n",
      "Deleted: images\\temporary_images\\5053083175498_l.jpg\n",
      "Deleted: images\\temporary_images\\5053083266615_m.jpg\n",
      "Deleted: images\\temporary_images\\5060000705089_l.jpg\n",
      "Deleted: images\\temporary_images\\8717418552565_l.jpg\n",
      "The formatted price is: £7.00\n",
      "An error occurred: Message: element click intercepted: Element <span class=\"text-sm md-text-base font-semibold grey-1000-color cx-link cursor-pointer\">...</span> is not clickable at point (665, 707). Other element would receive the click: <div class=\"store-list-container\">...</div>\n",
      "  (Session info: chrome=129.0.6668.90)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6999EB645+29573]\n",
      "\t(No symbol) [0x00007FF699960470]\n",
      "\t(No symbol) [0x00007FF69981B6EA]\n",
      "\t(No symbol) [0x00007FF6998774EE]\n",
      "\t(No symbol) [0x00007FF699874F3C]\n",
      "\t(No symbol) [0x00007FF699872408]\n",
      "\t(No symbol) [0x00007FF69987161A]\n",
      "\t(No symbol) [0x00007FF6998636BE]\n",
      "\t(No symbol) [0x00007FF6998972FA]\n",
      "\t(No symbol) [0x00007FF699862FF6]\n",
      "\t(No symbol) [0x00007FF699897510]\n",
      "\t(No symbol) [0x00007FF6998B86BC]\n",
      "\t(No symbol) [0x00007FF6998970A3]\n",
      "\t(No symbol) [0x00007FF6998612DF]\n",
      "\t(No symbol) [0x00007FF699862441]\n",
      "\tGetHandleVerifier [0x00007FF699D1C58D+3375821]\n",
      "\tGetHandleVerifier [0x00007FF699D67987+3684039]\n",
      "\tGetHandleVerifier [0x00007FF699D5CDAB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF699AAB7C6+816390]\n",
      "\t(No symbol) [0x00007FF69996B77F]\n",
      "\t(No symbol) [0x00007FF6999675A4]\n",
      "\t(No symbol) [0x00007FF699967740]\n",
      "\t(No symbol) [0x00007FF69995659F]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "An error occurred: Message: element click intercepted: Element <span class=\"text-sm md-text-base font-semibold grey-1000-color cx-link cursor-pointer\">...</span> is not clickable at point (665, 707). Other element would receive the click: <div class=\"store-list-container\">...</div>\n",
      "  (Session info: chrome=129.0.6668.90)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6999EB645+29573]\n",
      "\t(No symbol) [0x00007FF699960470]\n",
      "\t(No symbol) [0x00007FF69981B6EA]\n",
      "\t(No symbol) [0x00007FF6998774EE]\n",
      "\t(No symbol) [0x00007FF699874F3C]\n",
      "\t(No symbol) [0x00007FF699872408]\n",
      "\t(No symbol) [0x00007FF69987161A]\n",
      "\t(No symbol) [0x00007FF6998636BE]\n",
      "\t(No symbol) [0x00007FF6998972FA]\n",
      "\t(No symbol) [0x00007FF699862FF6]\n",
      "\t(No symbol) [0x00007FF699897510]\n",
      "\t(No symbol) [0x00007FF6998B86BC]\n",
      "\t(No symbol) [0x00007FF6998970A3]\n",
      "\t(No symbol) [0x00007FF6998612DF]\n",
      "\t(No symbol) [0x00007FF699862441]\n",
      "\tGetHandleVerifier [0x00007FF699D1C58D+3375821]\n",
      "\tGetHandleVerifier [0x00007FF699D67987+3684039]\n",
      "\tGetHandleVerifier [0x00007FF699D5CDAB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF699AAB7C6+816390]\n",
      "\t(No symbol) [0x00007FF69996B77F]\n",
      "\t(No symbol) [0x00007FF6999675A4]\n",
      "\t(No symbol) [0x00007FF699967740]\n",
      "\t(No symbol) [0x00007FF69995659F]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "An error occurred: Message: element click intercepted: Element <span class=\"text-sm md-text-base font-semibold grey-1000-color cx-link cursor-pointer\">...</span> is not clickable at point (665, 707). Other element would receive the click: <div class=\"store-list-container\">...</div>\n",
      "  (Session info: chrome=129.0.6668.90)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6999EB645+29573]\n",
      "\t(No symbol) [0x00007FF699960470]\n",
      "\t(No symbol) [0x00007FF69981B6EA]\n",
      "\t(No symbol) [0x00007FF6998774EE]\n",
      "\t(No symbol) [0x00007FF699874F3C]\n",
      "\t(No symbol) [0x00007FF699872408]\n",
      "\t(No symbol) [0x00007FF69987161A]\n",
      "\t(No symbol) [0x00007FF6998636BE]\n",
      "\t(No symbol) [0x00007FF6998972FA]\n",
      "\t(No symbol) [0x00007FF699862FF6]\n",
      "\t(No symbol) [0x00007FF699897510]\n",
      "\t(No symbol) [0x00007FF6998B86BC]\n",
      "\t(No symbol) [0x00007FF6998970A3]\n",
      "\t(No symbol) [0x00007FF6998612DF]\n",
      "\t(No symbol) [0x00007FF699862441]\n",
      "\tGetHandleVerifier [0x00007FF699D1C58D+3375821]\n",
      "\tGetHandleVerifier [0x00007FF699D67987+3684039]\n",
      "\tGetHandleVerifier [0x00007FF699D5CDAB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF699AAB7C6+816390]\n",
      "\t(No symbol) [0x00007FF69996B77F]\n",
      "\t(No symbol) [0x00007FF6999675A4]\n",
      "\t(No symbol) [0x00007FF699967740]\n",
      "\t(No symbol) [0x00007FF69995659F]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "An error occurred: Message: element click intercepted: Element <span class=\"text-sm md-text-base font-semibold grey-1000-color cx-link cursor-pointer\">...</span> is not clickable at point (665, 707). Other element would receive the click: <div class=\"store-list-container\">...</div>\n",
      "  (Session info: chrome=129.0.6668.90)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6999EB645+29573]\n",
      "\t(No symbol) [0x00007FF699960470]\n",
      "\t(No symbol) [0x00007FF69981B6EA]\n",
      "\t(No symbol) [0x00007FF6998774EE]\n",
      "\t(No symbol) [0x00007FF699874F3C]\n",
      "\t(No symbol) [0x00007FF699872408]\n",
      "\t(No symbol) [0x00007FF69987161A]\n",
      "\t(No symbol) [0x00007FF6998636BE]\n",
      "\t(No symbol) [0x00007FF6998972FA]\n",
      "\t(No symbol) [0x00007FF699862FF6]\n",
      "\t(No symbol) [0x00007FF699897510]\n",
      "\t(No symbol) [0x00007FF6998B86BC]\n",
      "\t(No symbol) [0x00007FF6998970A3]\n",
      "\t(No symbol) [0x00007FF6998612DF]\n",
      "\t(No symbol) [0x00007FF699862441]\n",
      "\tGetHandleVerifier [0x00007FF699D1C58D+3375821]\n",
      "\tGetHandleVerifier [0x00007FF699D67987+3684039]\n",
      "\tGetHandleVerifier [0x00007FF699D5CDAB+3640043]\n",
      "\tGetHandleVerifier [0x00007FF699AAB7C6+816390]\n",
      "\t(No symbol) [0x00007FF69996B77F]\n",
      "\t(No symbol) [0x00007FF6999675A4]\n",
      "\t(No symbol) [0x00007FF699967740]\n",
      "\t(No symbol) [0x00007FF69995659F]\n",
      "\tBaseThreadInitThunk [0x00007FFAAFD97374+20]\n",
      "\tRtlUserThreadStart [0x00007FFAB163CC91+33]\n",
      "\n",
      "Not a valid product\n",
      "Not a valid product\n",
      "Not a valid product\n",
      "Not a valid product\n",
      "Not a valid product\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import smtplib\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import random\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import math\n",
    "import difflib\n",
    "import requests\n",
    "import os\n",
    "import cv2  # Import OpenCV\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# Specify the path to the geckodriver executable\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "amazon_titles= []\n",
    "amazon_links = []\n",
    "webuy_titles = []\n",
    "webuy_links = []\n",
    "amazon_images = []\n",
    "webuy_images = []\n",
    "calculated_prices = []\n",
    "product_codes = []\n",
    "buy_prices = []\n",
    "\n",
    "\n",
    "sellers_black_list = [\" Live Life Retail\"]\n",
    "seller_white_list = [\" Selling-Prime-UK\"]\n",
    "with open(\"WorkingProxies.txt\", \"r\", encoding=\"utf-8\") as prxsfile:\n",
    "    proxy_list = prxsfile.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_random_proxy():\n",
    "    \"\"\"Select a random proxy from the list.\"\"\"\n",
    "    return random.choice(proxy_list)\n",
    "\n",
    "def create_driver_with_proxy(proxy):\n",
    "    \"\"\"Create a new Chrome WebDriver instance with a proxy.\"\"\"\n",
    "    # Split the proxy IP and port\n",
    "    proxy_ip, proxy_port = proxy.split(':')\n",
    "    \n",
    "    # Create Chrome options\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Configure proxy settings for both HTTP and HTTPS\n",
    "    proxy_settings = f\"{proxy_ip}:{proxy_port}\"\n",
    "    chrome_options.add_argument(f'--proxy-server={proxy_settings}')\n",
    "    \n",
    "    # Specify the path to the Chrome executable if needed (optional)\n",
    "    # chrome_options.binary_location = \"C:\\\\Path\\\\To\\\\Chrome.exe\"  # Only needed if Chrome is not in PATH\n",
    "    \n",
    "    # Initialize the WebDriver with the options\n",
    "    amazon_driver = webdriver.Chrome(options=chrome_options)\n",
    "    return amazon_driver\n",
    "\n",
    "def create_driver_without_proxy():\n",
    "    \"\"\"Create a new Chrome WebDriver instance without a proxy.\"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize the WebDriver with the options\n",
    "    driver = webdriver.Chrome()\n",
    "    return driver\n",
    "\n",
    "# Rotate proxies\n",
    "high_price = 31\n",
    "low_price = 25\n",
    "proxy = get_random_proxy()  # Get a random proxy\n",
    "#driver = create_driver_with_proxy(proxy)  # Create WebDriver with that proxy\n",
    "amazon_driver = create_driver_without_proxy()\n",
    "url = f\"https://www.amazon.co.uk/s?k=blu+ray&i=merchant-items&me=A1UFLUL80142N4&s=price-desc-rank&qid=1728216348&rnid=389127011&ref=sr_nr_p_36_0_0&low-price={low_price}&high-price={high_price}\"\n",
    "\n",
    "try:\n",
    "    amazon_driver.get(url)\n",
    "except Exception:\n",
    "    print(f\"Using proxy: {proxy}, BAD proxy\")\n",
    "# Define a function to wait for an element to be clickable\n",
    "\n",
    "def wait_for_element_to_be_clickable(driver, xpath, timeout=10):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath))\n",
    "    )\n",
    "amazon_cookies_PATH = '/html/body/div[2]/span/form/div[2]/span[1]/span/input'\n",
    "try:\n",
    "    # Wait until the element is clickable\n",
    "    WebDriverWait(amazon_driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, amazon_cookies_PATH))\n",
    "    )\n",
    "    amazon_driver.find_element(By.XPATH, amazon_cookies_PATH).click()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Click the \"Deliver to\" button\n",
    "deliver_to_button_PATH = '//*[@id=\"glow-ingress-line2\"]'\n",
    "wait_for_element_to_be_clickable(amazon_driver, deliver_to_button_PATH).click()\n",
    "\n",
    "# Click the Post Code input field\n",
    "PostCode_input_PATH = '//*[@id=\"GLUXZipUpdateInput\"]'\n",
    "wait_for_element_to_be_clickable(amazon_driver, PostCode_input_PATH).click()\n",
    "\n",
    "# Enter the postcode\n",
    "amazon_driver.find_element(By.XPATH, PostCode_input_PATH).send_keys(\"SW1A2aa\")\n",
    "\n",
    "# Click the \"Apply\" button\n",
    "Apply_button_PATH = '//*[@id=\"GLUXZipUpdate\"]/span/input'\n",
    "wait_for_element_to_be_clickable(amazon_driver, Apply_button_PATH).click()\n",
    "\n",
    "# Click the \"Continue\" button\n",
    "continue_button_PATH = '/html/body/div[6]/div/div/div[2]/span/span/input'\n",
    "wait_for_element_to_be_clickable(amazon_driver, continue_button_PATH).click()\n",
    "\n",
    "# Click to accept cookies\n",
    "accept_cookies_PATH = '/html/body/div[1]/span/form/div[2]/span[1]/span/input'\n",
    "wait_for_element_to_be_clickable(amazon_driver, accept_cookies_PATH).click()\n",
    "\n",
    "try:\n",
    "    amazon_driver.get(url)\n",
    "except Exception:\n",
    "    print(f\"Using proxy: {proxy}, BAD proxy\")\n",
    "\n",
    "\n",
    "# Define the XPATH of the target element (next page button)\n",
    "next_page_button_PATH = 's-pagination-item s-pagination-next s-pagination-button s-pagination-separator'\n",
    "\n",
    "# Define the scroll step (in pixels)\n",
    "scroll_step = 500\n",
    "\n",
    "# Scroll and search for the element\n",
    "tries = 20  # Number of tries before stopping\n",
    "i = 0  # Initialize attempt counter\n",
    "last_height = amazon_driver.execute_script(\"return document.body.scrollHeight\")  # Get the initial page height\n",
    "\n",
    "while i < tries:\n",
    "    try:\n",
    "        html_source = amazon_driver.page_source\n",
    "        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        # Try to find the element using its XPATH\n",
    "        next_button = soup.find_all('svg')[-1]['xmlns']\n",
    "        if next_button:\n",
    "            \n",
    "            break  # Exit the loop if the element is found\n",
    "        else:\n",
    "            # Scroll down incrementally by scroll_step\n",
    "            amazon_driver.execute_script(f\"window.scrollBy(0, {scroll_step});\")\n",
    "            \n",
    "            # Wait for the page to load new content\n",
    "            time.sleep(1)  # Adjust based on page loading speed\n",
    "            \n",
    "            # Get the new scroll height after scrolling\n",
    "            new_height = amazon_driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            # Check if the page has reached the end (no more content to scroll)\n",
    "            \n",
    "            # Update the last height\n",
    "            last_height = new_height\n",
    "            i += 1  # Increment attempt counter\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "Num_of_products = int(amazon_driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/span/div/h1/div/div[1]\").text.split(\"results\")[0].strip().split(\" \")[-1].replace(\",\", ''))\n",
    "print(Num_of_products)\n",
    "pages = 20 if Num_of_products / 16 >= 20 else  math.ceil(Num_of_products / 16)\n",
    "print(pages)\n",
    "products_link_tags = soup.find_all(class_=\"a-link-normal s-no-outline\")\n",
    "amazon_products_links = [\"https://www.amazon.co.uk/\"+products_link['href'] for products_link in products_link_tags]\n",
    "\n",
    "for amazon_products_link in amazon_products_links:\n",
    "    amazon_links.append(amazon_products_link)\n",
    "\n",
    "    product_code = amazon_products_link.split(\"/dp/\")[1].split(\"/\")[0] \n",
    "    product_codes.append(product_code)\n",
    "    amazon_driver.get(amazon_products_links[0])\n",
    "    valid_product = True\n",
    "    for i in range(0,5):\n",
    "        try: \n",
    "            html_source = amazon_driver.page_source\n",
    "            soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "                    # Try to find the element using its XPATH\n",
    "            priceTag = soup.find(id=\"corePriceDisplay_desktop_feature_div\")\n",
    "            if priceTag:\n",
    "                price= priceTag.find(class_=\"a-price aok-align-center reinventPricePriceToPayMargin priceToPay\").text\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "   \n",
    "    print(price)\n",
    "\n",
    "    other_sellers_button_PATH = 'a-price'\n",
    "    try:\n",
    "        # Wait until the element is clickable\n",
    "        WebDriverWait(amazon_driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, other_sellers_button_PATH))\n",
    "        )\n",
    "        amazon_driver.find_element(By.XPATH, other_sellers_button_PATH).click()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    for i in range(0,5):\n",
    "        try: \n",
    "            html_source = amazon_driver.page_source\n",
    "            soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "            slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "            if slider:\n",
    "                amazon_sellerstags = slider.find_all(class_=\"a-size-small a-link-normal\")\n",
    "                amazon_pricestags = slider.find_all(class_=\"a-section a-spacing-none aok-align-center aok-relative\")\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    \n",
    "\n",
    "    amazon_sellers = []\n",
    "    for seller in amazon_sellerstags:\n",
    "        try:\n",
    "            # Check if seller is valid and has the required aria-label attribute\n",
    "            if seller[\"aria-label\"] == \"Opens a new page\":\n",
    "                amazon_sellers.append(seller.text)  # Add to the list if conditions are met\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions (e.g., element doesn't support get_attribute)\n",
    "            print(e)  # You can log the exception if needed or just pass\n",
    "\n",
    "\n",
    "    def check_blacklisted_sellers(sellers_list, sellers_black_list):\n",
    "        # Find the blacklisted sellers in the sellers list\n",
    "        blacklisted_sellers = [seller for seller in sellers_list if seller in sellers_black_list]\n",
    "\n",
    "        if blacklisted_sellers:\n",
    "            valid_product = False\n",
    "            print(\"Not a valid product\")\n",
    "        else:\n",
    "            print(\"No sellers in the list are blacklisted.\")\n",
    "    \n",
    "    def check_whitelisted_sellers(sellers_list, seller_white_list):\n",
    "        # Loop through the sellers list to find the first whitelisted seller\n",
    "        for index, seller in enumerate(sellers_list):\n",
    "            if seller in seller_white_list:\n",
    "                print(\"First whitelisted seller found at index:\", index)\n",
    "                return index  # Return the index of the first whitelisted seller\n",
    "        \n",
    "        print(\"No sellers in the list are whitelisted.\")\n",
    "        return -1  # Return -1 if no whitelisted seller is found\n",
    "\n",
    "\n",
    "\n",
    "    # Example usage\n",
    "    Wseller_index = check_whitelisted_sellers(amazon_sellers, seller_white_list)\n",
    "    if Wseller_index == -1:\n",
    "        valid_product = False\n",
    "    sellers20pounds_counter = 0 \n",
    "    amazon_prices = []\n",
    "    for p in amazon_pricestags:\n",
    "        priceSTR = p.find(class_='aok-offscreen').text.strip().split(\" \")[0].replace(\"£\", \"\")\n",
    "        riceNMR = float(priceSTR.replace(\"£\", \"\"))\n",
    "        amazon_prices.append(riceNMR)\n",
    "        if riceNMR <= 20.00:\n",
    "            sellers20pounds_counter += 1\n",
    "    if sellers20pounds_counter >= 2:\n",
    "        valid_product = False\n",
    "        print(\"Not a valid product\")\n",
    "    amazon_price = amazon_prices [Wseller_index]\n",
    "    percentage = 0.01\n",
    "\n",
    "    # Calculate 1% of the price\n",
    "    reduction = amazon_price * percentage\n",
    "\n",
    "    # Calculate the new price\n",
    "    calculated_price = amazon_price - reduction\n",
    "    calculated_prices.append(calculated_price)\n",
    "    print(\"Price after reducing 1% from\", amazon_price, \"is\", calculated_price)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # Removing duplicates while maintaining order using a set\n",
    "    # unique_sellers = []\n",
    "    # seen_sellers = set()\n",
    "    # for seller in sellers:\n",
    "    #     if seller not in seen_sellers:\n",
    "    #         unique_sellers.append(seller)\n",
    "    #         seen_sellers.add(seller)\n",
    "\n",
    "    # unique_prices = []\n",
    "    # seen_prices = set()\n",
    "    # for price in prices:\n",
    "    #     if price not in seen_prices:\n",
    "    #         unique_prices.append(price)\n",
    "    #         seen_prices.add(price)\n",
    "\n",
    "    # print(unique_sellers)  # This will maintain the original order of sellers\n",
    "    # print(unique_prices)   # This will maintain the original order of prices\n",
    "\n",
    "    html_source = amazon_driver.page_source\n",
    "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    product_title = soup.find(id=\"productTitle\").text.strip()\n",
    "    amazon_titles.append(product_title)\n",
    "    product_image_tag = soup.find(id='imgTagWrapperId')\n",
    "    product_image = product_image_tag.find('img')['src']\n",
    "    webuy_driver = create_driver_without_proxy()\n",
    "\n",
    "    webuy_url = f\"https://uk.webuy.com/search?stext={product_title.replace(' ', '+').replace('&', 'and')}\"\n",
    "    webuy_driver.get(webuy_url)\n",
    "    time.sleep(5)\n",
    "    html_source = webuy_driver.page_source\n",
    "    webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    for _ in range(0, 5):\n",
    "        html_source = webuy_driver.page_source\n",
    "        webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "\n",
    "        cookies = webuy_soup.find(id=\"onetrust-accept-btn-handler\")\n",
    "        cookies_path = '/html/body/div[3]/div[2]/div/div/div[2]/div/div/button'\n",
    "        if cookies:\n",
    "            print(\"cookies found\")\n",
    "            wait_for_element_to_be_clickable(webuy_driver, cookies_path).click()\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    for _ in range(0, 5):\n",
    "\n",
    "\n",
    "        html_source = webuy_driver.page_source\n",
    "        webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        results = webuy_soup.find(class_='search-result-grid mb-s md-mb-0')\n",
    "        if results:\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    html_source = webuy_driver.page_source\n",
    "    webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    results = webuy_soup.find(class_='search-result-grid mb-s md-mb-0')\n",
    "\n",
    "    webuy_products = results.find_all(class_='search-product-card')\n",
    "\n",
    "    webuy_product_titles = []\n",
    "    webuy_product_links = []\n",
    "    webuy_product_images = []\n",
    "    for product in webuy_products:\n",
    "        webuy_product_titles.append(product.find(class_='card-title').text)\n",
    "        webuy_product_links.append('https://uk.webuy.com' + product.find('a')['href'])\n",
    "        webuy_product_images.append(product.find('img')['src'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_similarity(reference_string, string_list):\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for string in string_list:\n",
    "            # Calculate similarity ratio\n",
    "            similarity_ratio = difflib.SequenceMatcher(None, reference_string, string).ratio()\n",
    "            # Convert to percentage\n",
    "            similarity_percentage = similarity_ratio * 100\n",
    "            similarity_scores.append((string, similarity_percentage))\n",
    "        \n",
    "        return similarity_scores\n",
    "\n",
    "    title_similarity_results = calculate_similarity(product_title, webuy_product_titles)\n",
    "\n",
    "    # Print results\n",
    "    for string, score in title_similarity_results:\n",
    "        print(f\"Similarity with '{string}': {score:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Libraries are installed correctly!\")\n",
    "\n",
    "    def download_images(image_urls, download_dir='images\\\\temporary_images'):\n",
    "        \"\"\"Download images from the given URLs.\"\"\"\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        downloaded_images = []\n",
    "        for url in image_urls:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()  # Raise an error for bad responses\n",
    "                image_name = os.path.join(download_dir, url.split(\"/\")[-1])\n",
    "                with open(image_name, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                downloaded_images.append(image_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "        return downloaded_images\n",
    "\n",
    "\n",
    "    def download_reference_image(url, download_dir='images\\\\reference'):\n",
    "        \"\"\"Download the reference image to a specified directory.\"\"\"\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an error for bad responses\n",
    "            image_name = os.path.join(download_dir, url.split(\"/\")[-1])\n",
    "            with open(image_name, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            return image_name\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading reference image {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def calculate_ssim(img1, img2):\n",
    "        \"\"\"Calculate SSIM manually.\"\"\"\n",
    "        # Constants to avoid division by zero\n",
    "        C1 = 6.5025\n",
    "        C2 = 58.5225\n",
    "\n",
    "        # Calculate means\n",
    "        mu1 = np.mean(img1)\n",
    "        mu2 = np.mean(img2)\n",
    "\n",
    "        # Calculate variances\n",
    "        sigma1_sq = np.var(img1)\n",
    "        sigma2_sq = np.var(img2)\n",
    "        sigma12 = np.cov(img1.flatten(), img2.flatten())[0][1]\n",
    "\n",
    "        # SSIM formula\n",
    "        ssim = ((2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)) / \\\n",
    "                ((mu1 ** 2 + mu2 ** 2 + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "        return ssim\n",
    "\n",
    "\n",
    "    def compare_images(image1_path, image2_path):\n",
    "        \"\"\"Compare two images and return similarity percentage using OpenCV.\"\"\"\n",
    "        # Read the images\n",
    "        img1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "        img2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "\n",
    "        # Resize images to the same size for comparison\n",
    "        img1 = cv2.resize(img1, (100, 100))  # Adjust size as necessary\n",
    "        img2 = cv2.resize(img2, (100, 100))  # Adjust size as necessary\n",
    "\n",
    "        # Calculate SSIM\n",
    "        ssim_score = calculate_ssim(img1, img2)\n",
    "        \n",
    "        # Convert to percentage\n",
    "        similarity_percentage = ssim_score * 100\n",
    "        return similarity_percentage\n",
    "\n",
    "\n",
    "    # Download images\n",
    "    downloaded_images = download_images(webuy_product_images)\n",
    "\n",
    "    # Download the reference image\n",
    "    reference_image_path = download_reference_image(product_image)\n",
    "    amazon_images.append(reference_image_path)\n",
    "    # Compare each downloaded image with the reference image if it was successfully downloaded\n",
    "    if reference_image_path:\n",
    "        image_similarity_results = []\n",
    "        for img_path in downloaded_images:\n",
    "            similarity = compare_images(reference_image_path, img_path)\n",
    "            image_similarity_results.append((img_path, similarity))\n",
    "\n",
    "        # Print results\n",
    "        for img_path, score in image_similarity_results:\n",
    "            print(f\"Similarity with '{img_path}': {score:.2f}%\")\n",
    "    else:\n",
    "        print(\"Reference image download failed; comparison will not be performed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_percentages(title_data, image_data):\n",
    "        results = []\n",
    "        \n",
    "        # Ensure both lists have the same length\n",
    "        if len(title_data) != len(image_data):\n",
    "            raise ValueError(\"The length of title_data and image_data must be the same.\")\n",
    "        \n",
    "        for (title, title_percentage), (image, image_percentage) in zip(title_data, image_data):\n",
    "            new_percentage = (0.8 * image_percentage) + (0.2 * title_percentage)\n",
    "            results.append(new_percentage)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages = calculate_percentages(title_similarity_results, image_similarity_results)\n",
    "\n",
    "\n",
    "    # Finding the highest percentage and its index\n",
    "    highest_percentage = max(percentages)\n",
    "    highest_index = percentages.index(highest_percentage)\n",
    "    webuy_image = image_similarity_results[percentages.index(highest_percentage)][0]\n",
    "    webuy_images.append(webuy_image)\n",
    "    webuy_link = webuy_product_links[highest_index]\n",
    "    webuy_driver.get(webuy_link)\n",
    "    webuy_links.append(webuy_link)\n",
    "    def clear_temp_images(download_dir='images\\\\temporary_images'):\n",
    "        \"\"\"Clear all images in the temporary images directory.\"\"\"\n",
    "        if os.path.exists(download_dir):\n",
    "            for filename in os.listdir(download_dir):\n",
    "                file_path = os.path.join(download_dir, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        if file_path != webuy_image:\n",
    "                            os.remove(file_path)  # Remove the file\n",
    "                            print(f\"Deleted: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"The directory {download_dir} does not exist.\")\n",
    "\n",
    "    # Example usage\n",
    "    clear_temp_images()\n",
    "    time.sleep(2)\n",
    "    for _ in range(0, 5):\n",
    "        html_source = webuy_driver.page_source\n",
    "        webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "\n",
    "        cookies = webuy_soup.find(id=\"onetrust-accept-btn-handler\")\n",
    "        cookies_path = '/html/body/div[3]/div[2]/div/div/div[2]/div/div/button'\n",
    "        if cookies:\n",
    "            print(\"cookies found\")\n",
    "            wait_for_element_to_be_clickable(webuy_driver, cookies_path).click()\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "    for _ in range(0, 5):\n",
    "        html_source = webuy_driver.page_source\n",
    "        webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        # Extracting the price\n",
    "        if webuy_soup.find(class_=\"sell-price md-mb-0 d-block w-100\") and webuy_soup.find(class_=\"heading-s-semibold md-heading-l-semibold\"):\n",
    "            webuy_priceSTR = webuy_soup.find(class_=\"sell-price md-mb-0 d-block w-100\").text.replace(\"£\", \"\")\n",
    "            webuy_title = webuy_soup.find(class_=\"heading-s-semibold md-heading-l-semibold\").text\n",
    "            break        \n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    # Converting the price to a float and adding 3\n",
    "    webuy_price = float(webuy_priceSTR) + 3\n",
    "    formatted_price = f\"£{webuy_price:.2f}\"\n",
    "\n",
    "    buy_prices.append(float(f\"{webuy_price:.2f}\"))\n",
    "    webuy_titles.append(webuy_title)\n",
    "    # Alternatively, using an f-string\n",
    "\n",
    "    # Printing the formatted price\n",
    "    print(\"The formatted price is:\", formatted_price)\n",
    "    for _ in range(0, 5):\n",
    "        check_store_button_PATH = '/html/body/div[1]/div/main/div/div/div[1]/div[1]/div[2]/div[4]/div[2]/div/span'\n",
    "\n",
    "        try:\n",
    "            # Wait until the element is clickable\n",
    "            WebDriverWait(webuy_driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, check_store_button_PATH))\n",
    "            )\n",
    "            webuy_driver.find_element(By.XPATH, check_store_button_PATH).click()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    for _ in range(0, 5):\n",
    "        \n",
    "        all_store_button_PATH = '/html/body/div[1]/div/main/div/div/div[6]/div[2]/button[2]/span'\n",
    "\n",
    "        try:\n",
    "            # Wait until the element is clickable\n",
    "            WebDriverWait(webuy_driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, all_store_button_PATH))\n",
    "            )\n",
    "            webuy_driver.find_element(By.XPATH, all_store_button_PATH).click()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            time.sleep(0.5)\n",
    "    html_source = webuy_driver.page_source\n",
    "    webuy_soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "    store_name_class = 'store-name d-flex justify-content-between'\n",
    "    for _ in range(0, 5):\n",
    "        if len(webuy_soup.find_all(class_=store_name_class)) < 3:\n",
    "            valid_product = False\n",
    "            print(\"Not a valid product\")\n",
    "\n",
    "\n",
    "    store = webuy_soup.find_all(class_=store_name_class)\n",
    "    comparing_price = webuy_price *2\n",
    "    cheap_sellers = 0\n",
    "    for price in amazon_prices:\n",
    "        if price < comparing_price:\n",
    "            cheap_sellers += 1\n",
    "\n",
    "    if cheap_sellers >= 3:\n",
    "        valid_product = False\n",
    "        print(\"Not a valid product\")\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl.drawing.image import Image\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "data = {\n",
    "    'Amazon_title': amazon_titles,\n",
    "    'Amazon_link': amazon_links,\n",
    "    'Webuy_title': webuy_titles,\n",
    "    'Webuy_link': webuy_links,\n",
    "    'Amazon_image': amazon_images,\n",
    "    'Webuy_image': webuy_images,\n",
    "    'Calculated_price': calculated_prices,\n",
    "    'Product_code': product_codes,\n",
    "    'Buy_price': buy_prices,\n",
    "}\n",
    "\n",
    "# Create a new workbook and select the active worksheet\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Products\"\n",
    "\n",
    "# Write headers\n",
    "headers = ['Amazon Title', 'Amazon Link', 'Webuy Title', 'Webuy Link', 'Amazon Image', 'Webuy Image','Calculated Price', 'Product Code',  'Buy Price']\n",
    "ws.append(headers)\n",
    "\n",
    "# Define styles for headers\n",
    "header_fill = PatternFill(start_color=\"808080\", end_color=\"808080\", fill_type=\"solid\")  # Gray fill\n",
    "header_font = Font(bold=True)  # Bold font\n",
    "\n",
    "# Apply styles to headers\n",
    "for col_num, header in enumerate(headers, start=1):\n",
    "    cell = ws.cell(row=1, column=col_num)\n",
    "    cell.fill = header_fill\n",
    "    cell.font = header_font\n",
    "\n",
    "# Set column widths\n",
    "column_widths = [30, 20, 30, 20, 10, 10, 15, 15, 15]  # Customize widths as needed\n",
    "for col_num, width in enumerate(column_widths, start=1):\n",
    "    ws.column_dimensions[openpyxl.utils.get_column_letter(col_num)].width = width\n",
    "\n",
    "# Add data and images\n",
    "for i in range(len(data['Amazon_title'])):\n",
    "    # Append Amazon Title\n",
    "    ws.cell(row=i + 2, column=1, value=data['Amazon_title'][i])  # Amazon Title\n",
    "\n",
    "    # Create clickable hyperlink for Amazon link\n",
    "    amazon_cell = ws.cell(row=i + 2, column=2)\n",
    "    amazon_cell.value = \"Amazon Link\"  # Set display text\n",
    "    amazon_cell.hyperlink = data['Amazon_link'][i]  # Directly assign the link\n",
    "\n",
    "    # Append Webuy Title\n",
    "    ws.cell(row=i + 2, column=3, value=data['Webuy_title'][i])  # Webuy Title\n",
    "\n",
    "    # Create clickable hyperlink for Webuy link\n",
    "    webuy_cell = ws.cell(row=i + 2, column=4)\n",
    "    webuy_cell.value = \"Webuy Link\"  # Set display text\n",
    "    webuy_cell.hyperlink = data['Webuy_link'][i]  # Directly assign the link\n",
    "\n",
    "    # Load and insert the Amazon image\n",
    "    amazon_img_path = data['Amazon_image'][i]\n",
    "    amazon_img = Image(amazon_img_path)\n",
    "    amazon_img.width = 50  # Set width\n",
    "    amazon_img.height = 50  # Set height\n",
    "    amazon_img.anchor = f'E{i + 2}'  # Column E for Amazon Images\n",
    "    ws.add_image(amazon_img)\n",
    "\n",
    "    # Load and insert the Webuy image\n",
    "    webuy_img_path = data['Webuy_image'][i]\n",
    "    webuy_img = Image(webuy_img_path)\n",
    "    webuy_img.width = 50  # Set width\n",
    "    webuy_img.height = 50  # Set height\n",
    "    webuy_img.anchor = f'F{i + 2}'  # Column F for Webuy Images\n",
    "    ws.add_image(webuy_img)\n",
    "\n",
    "    # Set row height to fit the images\n",
    "    ws.row_dimensions[i + 2].height = amazon_img.height  # Set height based on Amazon image\n",
    "\n",
    "    # Product Code, Calculated Price, and Buy Price\n",
    "    ws.cell(row=i + 2, column=7, value=f\"£{data['Calculated_price'][i]:.2f}\")  # Calculated Price\n",
    "    ws.cell(row=i + 2, column=8, value=data['Product_code'][i])  # Product Code\n",
    "    ws.cell(row=i + 2, column=9, value=f\"£{data['Buy_price'][i]:.2f}\")  # Buy Price\n",
    "\n",
    "    # Fill data cells with color\n",
    "    for col_num in range(1, len(headers) + 1):\n",
    "        cell = ws.cell(row=i + 2, column=col_num)\n",
    "        cell.fill = PatternFill(start_color=\"D3D3D3\", end_color=\"D3D3D3\", fill_type=\"solid\")  # Light gray fill\n",
    "\n",
    "# Save the workbook\n",
    "wb.save('final_sheet.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webuy_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import time\n",
    "\n",
    "# Step 1: Move the mouse to the pixel coordinates and click\n",
    "x = 1415  # Replace with the X coordinate of the pixel\n",
    "y = 406  # Replace with the Y coordinate of the pixel\n",
    "\n",
    "# Move the mouse to the (x, y) position and click\n",
    "pyautogui.moveTo(x, y)\n",
    "pyautogui.click()\n",
    "\n",
    "# Optional: You can add a delay before sending keys\n",
    "time.sleep(1)\n",
    "\n",
    "# Step 2: Send keys (simulates typing a string)\n",
    "pyautogui.write(\"29\", interval=0.1)  # Types 'Hello World' with a slight delay between each character\n",
    "\n",
    "# Step 3: Press individual keys (simulates key presses)\n",
    "pyautogui.press('enter')  # Simulates pressing the 'Enter' key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    try: \n",
    "        html_source = amazon_driver.page_source\n",
    "        soup = BeautifulSoup(html_source, \"html.parser\")\n",
    "        slider = soup.find(id=\"all-offers-display-scroller\")\n",
    "        if slider:\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "amazon_sellerstags = slider.find_all(class_=\"a-size-small a-link-normal\")\n",
    "amazon_pricestags = slider.find_all(class_=\"a-section a-spacing-none aok-align-center aok-relative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Selling-Prime-UK',\n",
       " ' Selling-Prime-UK',\n",
       " ' Brit_Books',\n",
       " ' musicMagpie',\n",
       " ' moviemars-uk',\n",
       " ' Davina Jayne Ltd',\n",
       " ' Chalkys UK',\n",
       " ' Marastore',\n",
       " ' RAREWAVES',\n",
       " ' Red Pebble Products',\n",
       " ' reflexcd2 SHIPPING FROM THE UK']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "webuy_driver = create_driver_without_proxy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Incredibles, The - 2 Disc SE (U) 2004'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
